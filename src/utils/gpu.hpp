#pragma once

#include <cassert>
#include <cstdio>
#include <cstdlib>

#ifdef O_HIP

#include <hip/hip_runtime.h>

#if defined(CUFFT) || defined(PARIS)

#include <hipfft.h>

static void __attribute__((unused)) check(const hipfftResult err, const char *const file, const int line)
{
  if (err == HIPFFT_SUCCESS) return;
  fprintf(stderr,"HIPFFT ERROR AT LINE %d OF FILE '%s': %d\n",line,file,err);
  fflush(stderr);
  exit(err);
}

#endif

#define CUFFT_D2Z HIPFFT_D2Z
#define CUFFT_FORWARD HIPFFT_FORWARD
#define CUFFT_INVERSE HIPFFT_BACKWARD
#define CUFFT_Z2D HIPFFT_Z2D
#define CUFFT_Z2Z HIPFFT_Z2Z

#define cudaDeviceSynchronize hipDeviceSynchronize
#define cudaError hipError_t
#define cudaError_t hipError_t
#define cudaErrorInsufficientDriver hipErrorInsufficientDriver
#define cudaErrorNoDevice hipErrorNoDevice
#define cudaEvent_t hipEvent_t
#define cudaEventCreate hipEventCreate
#define cudaEventElapsedTime hipEventElapsedTime
#define cudaEventRecord hipEventRecord
#define cudaEventSynchronize hipEventSynchronize
#define cudaFree hipFree
#define cudaFreeHost hipHostFree
#define cudaGetDevice hipGetDevice
#define cudaGetDeviceCount hipGetDeviceCount
#define cudaGetErrorString hipGetErrorString
#define cudaGetLastError hipGetLastError
#define cudaHostAlloc hipHostMalloc
#define cudaHostAllocDefault hipHostMallocDefault
#define cudaMalloc hipMalloc
#define cudaMemcpy hipMemcpy
#define cudaMemcpyAsync hipMemcpyAsync
#define cudaMemcpyDeviceToHost hipMemcpyDeviceToHost
#define cudaMemcpyDeviceToDevice hipMemcpyDeviceToDevice
#define cudaMemcpyHostToDevice hipMemcpyHostToDevice
#define cudaMemGetInfo hipMemGetInfo
#define cudaMemset hipMemset
#define cudaReadModeElementType hipReadModeElementType
#define cudaSetDevice hipSetDevice
#define cudaSuccess hipSuccess

#define cufftDestroy hipfftDestroy
#define cufftDoubleComplex hipfftDoubleComplex
#define cufftDoubleReal hipfftDoubleReal
#define cufftExecD2Z hipfftExecD2Z
#define cufftExecZ2D hipfftExecZ2D
#define cufftExecZ2Z hipfftExecZ2Z
#define cufftHandle hipfftHandle
#define cufftPlan3d hipfftPlan3d
#define cufftPlanMany hipfftPlanMany

static void __attribute__((unused)) check(const hipError_t err, const char *const file, const int line)
{
  if (err == hipSuccess) return;
  fprintf(stderr,"HIP ERROR AT LINE %d OF FILE '%s': %s %s\n",line,file,hipGetErrorName(err),hipGetErrorString(err));
  fflush(stderr);
  exit(err);
}

#else

#include <cuda_runtime.h>

#if defined(CUFFT) || defined(PARIS)

#include <cufft.h>

static void check(const cufftResult err, const char *const file, const int line)
{
  if (err == CUFFT_SUCCESS) return;
  fprintf(stderr,"CUFFT ERROR AT LINE %d OF FILE '%s': %d\n",line,file,err);
  fflush(stderr);
  exit(err);
}

#endif

static void check(const cudaError_t err, const char *const file, const int line)
{
  if (err == cudaSuccess) return;
  fprintf(stderr,"CUDA ERROR AT LINE %d OF FILE '%s': %s %s\n",line,file,cudaGetErrorName(err),cudaGetErrorString(err));
  fflush(stderr);
  exit(err);
}

#define hipLaunchKernelGGL(F,G,B,M,S,...) F<<<G,B,M,S>>>(__VA_ARGS__)

#endif

#define CHECK(X) check(X,__FILE__,__LINE__)

#define GPU_MAX_THREADS 256

#if defined(__CUDACC__) || defined(__HIPCC__)

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun0(const int n0, const F f)
{
  const int i0 = blockIdx.x*blockDim.x+threadIdx.x;
  if (i0 < n0) f(i0);
}

template <typename F>
void gpuFor(const int n0, const F f)
{
  if (n0 <= 0) return;
  const int b0 = (n0+GPU_MAX_THREADS-1)/GPU_MAX_THREADS;
  const int t0 = (n0+b0-1)/b0;
  gpuRun0<<<b0,t0>>>(n0,f);
  CHECK(cudaGetLastError());
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun0x2(const F f)
{
  const int i0 = threadIdx.y;
  const int i1 = threadIdx.x;
  f(i0,i1);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun1x1(const F f)
{
  const int i0 = blockIdx.x;
  const int i1 = threadIdx.x;
  f(i0,i1);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun2x0(const int n1, const F f)
{
  const int i0 = blockIdx.y;
  const int i1 = blockIdx.x*blockDim.x+threadIdx.x;
  if (i1 < n1) f(i0,i1);
}

template <typename F>
void gpuFor(const int n0, const int n1, const F f)
{
  if ((n0 <= 0) || (n1 <= 0)) return;
  const long nl01 = long(n0)*long(n1);
  assert(nl01 < long(INT_MAX));

  if (n1 > GPU_MAX_THREADS) {
    const int b1 = (n1+GPU_MAX_THREADS-1)/GPU_MAX_THREADS;
    const int t1 = (n1+b1-1)/b1;
    gpuRun2x0<<<dim3(b1,n0),dim3(t1)>>>(n1,f);
    CHECK(cudaGetLastError());
  } else if (nl01 > GPU_MAX_THREADS) {
    gpuRun1x1<<<n0,n1>>>(f);
    CHECK(cudaGetLastError());
  } else {
    gpuRun0x2<<<1,dim3(n1,n0)>>>(f);
    CHECK(cudaGetLastError());
  }
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun0x3(const F f)
{
  const int i0 = threadIdx.z;
  const int i1 = threadIdx.y;
  const int i2 = threadIdx.x;
  f(i0,i1,i2);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun1x2(const F f)
{
  const int i0 = blockIdx.x;
  const int i1 = threadIdx.y;
  const int i2 = threadIdx.x;
  f(i0,i1,i2);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun2x1(const F f)
{
  const int i0 = blockIdx.y;
  const int i1 = blockIdx.x;
  const int i2 = threadIdx.x;
  f(i0,i1,i2);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun3x0(const int n2, const F f)
{
  const int i0 = blockIdx.z;
  const int i1 = blockIdx.y;
  const int i2 = blockIdx.x*blockDim.x+threadIdx.x;
  if (i2 < n2) f(i0,i1,i2);
}

template <typename F>
void gpuFor(const int n0, const int n1, const int n2, const F f)
{
  if ((n0 <= 0) || (n1 <= 0) || (n2 <= 0)) return;
  const long nl12 = long(n1)*long(n2);
  const long nl012 = long(n0)*nl12;
  assert(nl012 < long(INT_MAX));

  if (n2 > GPU_MAX_THREADS) {
    const int b2 = (n2+GPU_MAX_THREADS-1)/GPU_MAX_THREADS;
    const int t2 = (n2+b2-1)/b2;
    gpuRun3x0<<<dim3(b2,n1,n0),t2>>>(n2,f);
    CHECK(cudaGetLastError());
  } else if (nl12 > GPU_MAX_THREADS) {
    gpuRun2x1<<<dim3(n1,n0),n2>>>(f);
    CHECK(cudaGetLastError());
  } else if (nl012 > GPU_MAX_THREADS) {
    gpuRun1x2<<<n0,dim3(n2,n1)>>>(f);
    CHECK(cudaGetLastError());
  } else {
    gpuRun0x3<<<1,dim3(n2,n1,n0)>>>(f);
    CHECK(cudaGetLastError());
  }
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun1x3(const F f)
{
  const int i0 = blockIdx.x;
  const int i1 = threadIdx.z;
  const int i2 = threadIdx.y;
  const int i3 = threadIdx.x;
  f(i0,i1,i2,i3);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun2x2(const F f)
{
  const int i0 = blockIdx.y;
  const int i1 = blockIdx.x;
  const int i2 = threadIdx.y;
  const int i3 = threadIdx.x;
  f(i0,i1,i2,i3);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun3x1(const F f)
{
  const int i0 = blockIdx.z;
  const int i1 = blockIdx.y;
  const int i2 = blockIdx.x;
  const int i3 = threadIdx.x;
  f(i0,i1,i2,i3);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun4x0(const int n23, const int n3, const F f)
{
  const int i23 = blockIdx.x*blockDim.x+threadIdx.x;
  if (i23 < n23) {
    const int i0 = blockIdx.z;
    const int i1 = blockIdx.y;
    const int i2 = i23/n3;
    const int i3 = i23%n3;
    f(i0,i1,i2,i3);
  }
}

template <typename F>
void gpuFor(const int n0, const int n1, const int n2, const int n3, const F f)
{
  if ((n0 <= 0) || (n1 <= 0) || (n2 <= 0) || (n3 <= 0)) return;
  const long nl23 = long(n2)*long(n3);
  const long nl123 = long(n1)*nl23;
  assert(long(n0)*nl123 < long(INT_MAX));

  const int n23 = int(nl23);
  const int n123 = int(nl123);
  if (n3 > GPU_MAX_THREADS) {
    const int b23 = (n23+GPU_MAX_THREADS-1)/GPU_MAX_THREADS;
    const int t23 = (n23+b23-1)/b23;
    gpuRun4x0<<<dim3(b23,n1,n0),t23>>>(n23,n3,f);
    CHECK(cudaGetLastError());
  } else if (n23 > GPU_MAX_THREADS) {
    gpuRun3x1<<<dim3(n2,n1,n0),n3>>>(f);
    CHECK(cudaGetLastError());
  } else if (n123 > GPU_MAX_THREADS) {
    gpuRun2x2<<<dim3(n1,n0),dim3(n3,n2)>>>(f);
    CHECK(cudaGetLastError());
  } else {
    gpuRun1x3<<<n0,dim3(n3,n2,n1)>>>(f);
    CHECK(cudaGetLastError());
  }
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun2x3(const F f)
{
  const int i0 = blockIdx.y;
  const int i1 = blockIdx.x;
  const int i2 = threadIdx.z;
  const int i3 = threadIdx.y;
  const int i4 = threadIdx.x;
  f(i0,i1,i2,i3,i4);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun3x2(const F f)
{
  const int i0 = blockIdx.z;
  const int i1 = blockIdx.y;
  const int i2 = blockIdx.x;
  const int i3 = threadIdx.y;
  const int i4 = threadIdx.x;
  f(i0,i1,i2,i3,i4);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun4x1(const int n1, const F f)
{
  const int i01 = blockIdx.z;
  const int i0 = i01/n1;
  const int i1 = i01%n1;
  const int i2 = blockIdx.y;
  const int i3 = blockIdx.x;
  const int i4 = threadIdx.x;
  f(i0,i1,i2,i3,i4);
}

template <typename F>
__global__ __launch_bounds__(GPU_MAX_THREADS)
void gpuRun5x0(const int n1, const int n34, const int n4, const F f)
{
  const int i34 = blockIdx.x*blockDim.x+threadIdx.x;
  if (i34 < n34) {
    const int i01 = blockIdx.z;
    const int i0 = i01/n1;
    const int i1 = i01%n1;
    const int i2 = blockIdx.y;
    const int i3 = i34/n4;
    const int i4 = i34%n4;
    f(i0,i1,i2,i3,i4);
  }
}

template <typename F>
void gpuFor(const int n0, const int n1, const int n2, const int n3, const int n4, const F f)
{
  if ((n0 <= 0) || (n1 <= 0) || (n2 <= 0) || (n3 <= 0) || (n4 <= 0)) return;
  const long nl01 = long(n0)*long(n1);
  const long nl34 = long(n3)*long(n4);
  assert(nl01*long(n2)*nl34 < long(INT_MAX));

  const int n34 = int(nl34);
  if (n4 > GPU_MAX_THREADS) {
    const int n01 = int(nl01);
    const int b34 = (n34+GPU_MAX_THREADS-1)/GPU_MAX_THREADS;
    const int t34 = (n34+b34-1)/b34;
    gpuRun5x0<<<dim3(b34,n2,n01),t34>>>(n1,n34,n4,f);
    CHECK(cudaGetLastError());
  } else if (n34 > GPU_MAX_THREADS) {
    const int n01 = n0*n1;
    gpuRun4x1<<<dim3(n3,n2,n01),n4>>>(n1,f);
    CHECK(cudaGetLastError());
  } else if (n2*n34 > GPU_MAX_THREADS) {
    gpuRun3x2<<<dim3(n2,n1,n0),dim3(n4,n3)>>>(f);
    CHECK(cudaGetLastError());
  } else {
    gpuRun2x3<<<dim3(n1,n0),dim3(n4,n3,n2)>>>(f);
    CHECK(cudaGetLastError());
  }
}

#define GPU_LAMBDA [=] __device__

#endif
